# Bias Detection Guide

## Overview

Cognitive biases can compromise evaluation quality. This guide helps you identify and mitigate biases in both the sources you evaluate and your own evaluation process.

---

## Common Cognitive Biases

### 1. Confirmation Bias

**What It Is**:
Tendency to search for, interpret, or recall information that confirms pre-existing beliefs while ignoring contradictory evidence.

**How It Manifests in Source Evaluation**:
- ❌ Only reading sources that agree with current practices
- ❌ Dismissing contrary evidence too quickly
- ❌ Interpreting ambiguous evidence as supporting your view
- ❌ Remembering only the parts that confirm your beliefs

**How to Detect It**:
- Ask: "Am I looking for evidence I'm right or evidence of truth?"
- Check: Did I seriously consider contradictory sources?
- Note: Am I dismissing criticism without fair evaluation?

**How to Mitigate It**:
- ✅ Actively seek contradictory evidence
- ✅ Steel-man opposing views (make them stronger before evaluating)
- ✅ Ask: "What would prove me wrong?"
- ✅ Evaluate contrary evidence first, not last
- ✅ Use the 5-question framework systematically

**Example**:
```
Scenario: You believe 60% compaction is optimal

Confirmation Bias Response:
- Find one blog supporting 60% → "See, I was right!"
- Ignore three sources suggesting 50% or 70%

Bias-Free Response:
- Read all sources (50%, 60%, 70% recommendations)
- Evaluate evidence quality for each
- Test different thresholds in practice
- Choose based on data, not preference
```

---

### 2. Novelty Bias (Shiny Object Syndrome)

**What It Is**:
Overvaluing new information or techniques simply because they're new, while undervaluing existing proven approaches.

**How It Manifests**:
- ❌ "This new technique will solve everything!"
- ❌ Adopting new patterns without testing
- ❌ Dismissing current approaches as "old"
- ❌ Constantly switching tools/techniques

**How to Detect It**:
- Ask: "Am I excited because it's new or because it's better?"
- Check: Did I evaluate this as rigorously as existing approaches?
- Note: Am I dismissing my current approach unfairly?

**How to Mitigate It**:
- ✅ Apply same validation rigor to new and existing
- ✅ Ask: "What problem does this solve that current approach doesn't?"
- ✅ Test new vs old side-by-side
- ✅ Require evidence, not just novelty
- ✅ Remember: New ≠ Better

**Example**:
```
Scenario: New "Quantum Agent Framework" released

Novelty Bias Response:
- "This looks amazing!" → Adopt immediately
- Abandon current working system

Bias-Free Response:
- What does it do differently?
- Is there evidence it's better?
- Does it solve a real problem?
- Test alongside current approach
- Adopt only if proven superior
```

---

### 3. Authority Bias

**What It Is**:
Tendency to attribute greater accuracy to the opinion of an authority figure, regardless of actual evidence.

**How It Manifests**:
- ❌ "Expert X said it, so it must be true"
- ❌ Not questioning credentials
- ❌ Accepting claims without evidence from "authorities"
- ❌ Dismissing good ideas from non-authorities

**How to Detect It**:
- Ask: "Am I evaluating the evidence or the person?"
- Check: Would I accept this if from an unknown source?
- Note: Am I skipping validation because of who said it?

**How to Mitigate It**:
- ✅ Verify credentials (is this their expertise?)
- ✅ Evaluate evidence regardless of source
- ✅ Remember: Even experts can be wrong
- ✅ Check if expert is speaking within their domain
- ✅ Look for evidence, not just authority

**Important Note**:
Authority matters (Tier 1-4 hierarchy), but evidence matters more. Even Tier 1 should make sense and have backing.

**Example**:
```
Scenario: Famous AI researcher recommends technique

Authority Bias Response:
- "They're famous → must be right" → Adopt immediately

Bias-Free Response:
- Check: Is this their area of expertise? (AI researcher + Claude = relevant)
- Evaluate: What evidence supports this?
- Validate: Does it align with Tier 1 sources?
- Test: Does it work in practice?
- Decision: Based on evidence, not fame
```

---

### 4. Availability Bias (Recency Bias)

**What It Is**:
Overweighting recently acquired information or easily recalled examples.

**How It Manifests**:
- ❌ "I just read this, so it must be important"
- ❌ Overvaluing recent blog post vs older research
- ❌ Assuming latest technique is best
- ❌ Forgetting older but valid approaches

**How to Detect It**:
- Ask: "Am I prioritizing this because it's recent or because it's important?"
- Check: Did I review older authoritative sources too?
- Note: Is timing influencing my judgment?

**How to Mitigate It**:
- ✅ Compare new information to established sources
- ✅ Check publication dates but don't overweight recency
- ✅ Review both new and foundational sources
- ✅ Ask: "Is this still relevant?" not just "Is this new?"
- ✅ Build knowledge from foundation up, not just latest trends

**Example**:
```
Scenario: Yesterday's blog post vs 6-month-old Anthropic paper

Availability Bias Response:
- Blog is fresh in mind → Seems more relevant
- Anthropic paper is "old" → Seems outdated

Bias-Free Response:
- Check: Is Anthropic paper still current? (Yes, 6 months is recent)
- Evaluate: Which has stronger evidence?
- Verify: Does blog cite the paper?
- Decision: Anthropic paper (Tier 1) > Blog (Tier 3/4)
```

---

### 5. Sunk Cost Fallacy

**What It Is**:
Continuing an approach because of invested time/effort, even when evidence shows a better way exists.

**How It Manifests**:
- ❌ "We already built this, so we should keep using it"
- ❌ Resisting improvements because of prior investment
- ❌ Defending current approach despite better alternatives
- ❌ "But I spent so much time on this..."

**How to Detect It**:
- Ask: "Would I choose this approach if starting fresh today?"
- Check: Am I defending this because it works or because I built it?
- Note: Is past investment influencing current decision?

**How to Mitigate It**:
- ✅ Evaluate based on current value, not past effort
- ✅ Be willing to replace/improve your own work
- ✅ Ask: "What's best going forward?" not "What validates past work?"
- ✅ View improvements as success, not admission of failure
- ✅ Remember: Iteration is progress

**Example**:
```
Scenario: Built extensive custom system, Tier 1 recommends different approach

Sunk Cost Fallacy Response:
- "I spent weeks building this" → Reject new approach
- Defend current system despite evidence

Bias-Free Response:
- Acknowledge: Time invested doesn't make it optimal
- Evaluate: Is new approach actually better?
- Consider: Can I adapt rather than rebuild?
- Decide: Based on forward value, not past cost
- Act: Improve if evidence supports it
```

---

### 6. Bandwagon Effect

**What It Is**:
Adopting practices because many others are doing so, regardless of evidence.

**How It Manifests**:
- ❌ "Everyone's using this, so should we"
- ❌ Assuming popularity = quality
- ❌ Following trends without evaluation
- ❌ Fear of missing out (FOMO)

**How to Detect It**:
- Ask: "Am I evaluating this or just following the crowd?"
- Check: What's my evidence beyond popularity?
- Note: Am I worried about being left behind?

**How to Mitigate It**:
- ✅ Evaluate based on merit, not popularity
- ✅ Ask: "Does this solve MY problem?"
- ✅ Remember: Popular ≠ Correct
- ✅ Look for evidence, not just adoption numbers
- ✅ Be willing to use less popular but better solutions

**Example**:
```
Scenario: Framework X is trending on Twitter

Bandwagon Effect Response:
- "10,000 developers use it" → Must adopt

Bias-Free Response:
- Why are they using it?
- Does it solve problems I have?
- Is there evidence it's effective?
- Is it appropriate for my use case?
- Decision based on fit, not popularity
```

---

### 7. Dunning-Kruger Effect

**What It Is**:
Overconfidence in understanding when knowledge is actually limited; underconfidence when knowledge is deep.

**How It Manifests**:
- ❌ Early learning: "This is simple, I understand everything"
- ❌ Deep expertise: "This is so complex, maybe I don't understand"
- ❌ Dismissing complex topics as simple
- ❌ Over-trusting initial understanding

**How to Detect It**:
- Ask: "How much do I really know about this?"
- Check: Am I oversimplifying complex topics?
- Note: Is my confidence proportional to my knowledge?

**How to Mitigate It**:
- ✅ Seek expert opinion when uncertain
- ✅ Acknowledge knowledge limits
- ✅ Study deeply before claiming understanding
- ✅ Test assumptions
- ✅ Stay humble and curious

**Example**:
```
Scenario: Reading first article on agent architecture

Dunning-Kruger Response:
- "I get it now, this is straightforward"
- Dismiss detailed official docs as overthinking

Bias-Free Response:
- "I have basic understanding, need deeper study"
- Read official docs thoroughly
- Test understanding through implementation
- Acknowledge complexity
```

---

### 8. Anchoring Bias

**What It Is**:
Over-relying on first piece of information encountered (the "anchor") when making decisions.

**How It Manifests**:
- ❌ First source sets expectations for all others
- ❌ Initial number/metric influences all comparisons
- ❌ Difficulty updating beliefs after initial impression
- ❌ Treating first approach as baseline/default

**How to Detect It**:
- Ask: "Is my first source influencing how I view others?"
- Check: Would I judge differently if I read in reverse order?
- Note: Am I using first information as reference point unfairly?

**How to Mitigate It**:
- ✅ Review multiple sources before forming opinion
- ✅ Deliberately consider range of possibilities
- ✅ Question initial assumptions
- ✅ Evaluate each source independently
- ✅ Start with most authoritative source (Tier 1), not random first

**Example**:
```
Scenario: First blog says "compact at 80%", then find Anthropic research

Anchoring Bias Response:
- 80% is anchor → View 60% as "too early"
- Judge other recommendations against 80%

Bias-Free Response:
- Recognize: First source may not be best
- Evaluate: What does Tier 1 say?
- Test: Multiple thresholds
- Decide: Based on evidence, not first number seen
```

---

## Biases in Information Sources

### Common Source Biases

**1. Commercial Bias**
- Source has financial incentive
- Promotes product/service
- **Detection**: Check for sales pitch, affiliate links
- **Mitigation**: Verify claims with independent sources

**2. Ideological Bias**
- Source promotes specific philosophy
- Dismisses alternatives unfairly
- **Detection**: One-sided arguments, strawman opponents
- **Mitigation**: Seek balanced sources, check both sides

**3. Selection Bias**
- Cherry-picked examples
- Ignores contrary cases
- **Detection**: Only success stories, no failures mentioned
- **Mitigation**: Look for comprehensive analysis

**4. Survivorship Bias**
- Only shows successes, not failures
- "This worked for Company X" (ignores Companies Y, Z that failed)
- **Detection**: No mention of failures or limitations
- **Mitigation**: Ask about failure cases

**5. Publish or Perish Bias**
- Research designed to show novelty for publication
- Overstates significance
- **Detection**: Claims of revolutionary breakthrough
- **Mitigation**: Check replication studies, practical adoption

---

## Bias Detection Checklist

### For Your Evaluation

When evaluating, ask yourself:

**Confirmation Bias**:
- [ ] Did I seek contradictory evidence?
- [ ] Did I fairly consider opposing views?
- [ ] Am I finding reasons to confirm my preference?

**Novelty Bias**:
- [ ] Am I excited because it's new or because it's better?
- [ ] Did I evaluate this as rigorously as existing approaches?

**Authority Bias**:
- [ ] Am I evaluating evidence or the person?
- [ ] Would I accept this from unknown source?

**Availability Bias**:
- [ ] Is recency influencing my judgment?
- [ ] Did I check older authoritative sources?

**Sunk Cost Fallacy**:
- [ ] Would I choose this if starting fresh?
- [ ] Is past investment clouding judgment?

**Bandwagon Effect**:
- [ ] Am I following the crowd without evaluation?
- [ ] Is popularity my main evidence?

**Dunning-Kruger**:
- [ ] Do I truly understand this deeply enough?
- [ ] Am I oversimplifying complexity?

**Anchoring Bias**:
- [ ] Is my first source biasing later evaluation?
- [ ] Am I using inappropriate reference point?

### For Information Sources

When reviewing content, check for:

- [ ] Commercial motivation (selling something)
- [ ] Ideological bias (one-sided arguments)
- [ ] Selection bias (cherry-picked examples)
- [ ] Survivorship bias (only successes shown)
- [ ] Claims too good to be true
- [ ] Dismissal of alternatives without fair evaluation
- [ ] Overgeneralization from limited examples
- [ ] Lack of nuance or caveats

---

## Debiasing Techniques

### 1. Pre-Mortem

Before adopting, imagine it failed:
- "Why did this approach fail?"
- "What went wrong?"
- "What did we miss?"

Helps surface concerns you're ignoring due to confirmation bias.

### 2. Steel-Manning

Strengthen opposing views before evaluating:
- Make the best case for alternatives
- Don't strawman opponents
- Consider strongest counter-arguments

Helps overcome confirmation bias and ensure fair evaluation.

### 3. Outside View

Look at base rates and general patterns:
- "How often do new frameworks actually improve things?"
- "What's typical success rate for this type of approach?"
- "What do statistics say?"

Helps counter availability bias and over-optimism.

### 4. Red Team

Assign role of critic:
- Deliberately try to find problems
- Challenge every assumption
- Look for weaknesses

Can be you or ask someone else to critique.

### 5. Wait Period

Don't decide immediately:
- Sleep on it
- Review after 24-48 hours
- See if initial excitement fades

Helps counter novelty bias and recency effects.

### 6. Checklist

Use systematic framework (like the 5 questions):
- Forces comprehensive evaluation
- Reduces reliance on intuition
- Ensures nothing skipped

This entire validation skill is a debiasing checklist.

---

## Red Flags: High-Bias Sources

Watch for these warning signs:

### Language Red Flags
- "Revolutionary"
- "Game-changing"
- "Everyone is switching to..."
- "Old way is dead"
- "The only way to..."
- "Simple trick that..."
- "Experts don't want you to know..."

### Argument Red Flags
- No limitations mentioned
- No trade-offs discussed
- Only positive examples
- Strawman opponents
- Appeals to emotion
- Dismisses criticism without addressing it
- Claims certainty on uncertain topics

### Evidence Red Flags
- "In my experience..." (anecdotal)
- "Studies show..." (no citation)
- "Experts agree..." (unnamed experts)
- Single case study
- No comparison to alternatives
- No failure cases mentioned
- Results too good to be true

---

## Systematic Debiasing Process

### Before Evaluation
1. Acknowledge you have biases
2. Identify which biases you're prone to
3. Commit to systematic evaluation
4. Use the 5-question framework

### During Evaluation
1. Apply checklist thoroughly
2. Actively seek contradictory evidence
3. Check for bias red flags
4. Document assumptions you're making
5. Question your initial reactions

### After Evaluation
1. Review decision-making process
2. Check if biases influenced you
3. Sleep on significant decisions
4. Get second opinion if important
5. Be willing to update if new evidence emerges

---

## Example: Bias-Aware Evaluation

**Scenario**: Evaluating blog post about new agent pattern

**Biased Evaluation**:
```
- Sounds exciting! (novelty bias)
- Author seems smart (authority bias)
- Everyone's talking about it (bandwagon)
- Adopt immediately!
```

**Bias-Aware Evaluation**:
```
1. Awareness: "I'm excited because it's new - check for novelty bias"

2. Systematic Validation:
   - Authority: Who is the author? Credentials?
   - Evidence: What backing? Just opinion or data?
   - Consistency: Aligns with Tier 1 sources?
   - Practical: Solves real problem? Worth effort?
   - Recency: Current and applicable?

3. Bias Checks:
   - Novelty: Am I excited because it's new or better?
   - Bandwagon: Is popularity my evidence?
   - Confirmation: Did I seek contrary views?

4. Steel-Man Alternatives:
   - What's best case for current approach?
   - What might be better than this new pattern?

5. Red Flags:
   - Check language (revolutionary, game-changing?)
   - Check evidence (specific or vague?)
   - Check balance (limitations mentioned?)

6. Decision:
   - Based on evidence, not excitement
   - INVESTIGATE if promising but unproven
   - Document reasoning
```

---

## Summary

**Common Biases**:
1. Confirmation - Seeking confirming evidence
2. Novelty - Overvaluing new over proven
3. Authority - Over-trusting experts
4. Availability - Overweighting recent info
5. Sunk Cost - Defending past investments
6. Bandwagon - Following the crowd
7. Dunning-Kruger - Overconfidence when ignorant
8. Anchoring - Over-relying on first info

**Debiasing Techniques**:
- Use systematic frameworks (5 questions)
- Actively seek contradictory evidence
- Steel-man opposing views
- Check bias checklists
- Wait before deciding
- Get second opinions
- Document reasoning

**Goal**: Awareness + systematic process = bias-resistant evaluation
